{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BlockDropout(nn.Module):\n",
    "  def __init__(self,p:float=0.5,block_size:float=2):\n",
    "    super(BlockDropout,self).__init__()\n",
    "\n",
    "    assert p < 1 and p >= 0\n",
    "    self.p = p\n",
    "    self.block_size = block_size\n",
    "  def forward(self,x):\n",
    "    if self.training:\n",
    "      og_x = x\n",
    "      *B,C = x.shape\n",
    "      with torch.no_grad():\n",
    "        # include C candidate vectors-to-zero for every instance of a channel\n",
    "        dropout_mask = torch.rand(x.shape,device=x.device) > self.p # i.e. this is 1 if you're *keeping* a dim\n",
    "        N = C // self.block_size\n",
    "        R = C % self.block_size\n",
    "        dropout_mask[...,:N*self.block_size] = dropout_mask[...,:N].reshape(-1).repeat_interleave(self.block_size).view(*B,N*self.block_size)\n",
    "\n",
    "      return x * dropout_mask.clone() / (1-self.p)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_tensor = torch.zeros(3,5,7) # B,N,C\n",
    "\n",
    "BlockDropout(0.1,block_size=5)(demo_tensor)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
